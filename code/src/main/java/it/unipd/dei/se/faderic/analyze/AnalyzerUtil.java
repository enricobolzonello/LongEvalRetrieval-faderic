package it.unipd.dei.se.faderic.analyze;

import org.apache.commons.lang3.StringUtils;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.CharArraySet;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.WordlistLoader;
import org.apache.lucene.analysis.opennlp.tools.NLPPOSTaggerOp;
import org.apache.lucene.analysis.opennlp.tools.NLPSentenceDetectorOp;
import org.apache.lucene.analysis.opennlp.tools.NLPTokenizerOp;
import org.apache.lucene.analysis.synonym.SolrSynonymParser;
import org.apache.lucene.analysis.synonym.SynonymMap;
import org.apache.lucene.analysis.tokenattributes.*;
import org.apache.lucene.search.spell.PlainTextDictionary;

import opennlp.tools.postag.POSModel;
import opennlp.tools.sentdetect.SentenceModel;
import opennlp.tools.tokenize.TokenizerModel;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.Reader;
import java.io.StringReader;
import java.text.ParseException;
import java.util.ArrayList;
import java.util.List;

/**
 * The {@code AnalyzerUtil} class provides various utility methods for text analysis, including tokenization,
 * loading of synonym and dictionary lists, and loading of Apache OpenNLP models.
 */
public class AnalyzerUtil {

	private static final ClassLoader CL = AnalyzerUtil.class.getClassLoader();

	/**
	 * This function takes an analyzer and a string as input, tokenizes the string using the analyzer, and
	 * returns a list of the resulting tokens.
	 * 
	 * @param analyzer an instance of an {@code Analyzer} class that is used to tokenize the input string. An
	 * {@code Analyzer} is responsible for breaking down the input text into individual tokens or terms.
	 * @param string The input string that needs to be tokenized into individual words or terms.
	 * @return A list of strings that are the tokens generated by the given analyzer from the input
	 * string.
	 * @throws IOException if something goes wrong reading the {@code TokenStream}
	 */
	public static List<String> tokenizeString(Analyzer analyzer, String string) throws IOException {
		List<String> result = new ArrayList<String>();
		final TokenStream stream = analyzer.tokenStream(null, new StringReader(string));
		final CharTermAttribute tokenTerm = stream.addAttribute(CharTermAttribute.class);
		try {
			stream.reset();
			while (stream.incrementToken())
				result.add(tokenTerm.toString());
			stream.end();
		} finally {
			stream.close();
		}
		return result;
	}

	/**
	 * This function loads a synonym list from a file and returns a {@code SynonymMap} object.
	 * 
	 * @param synonymFile The name of the file containing the synonym list to be loaded.
	 * @param analyzer The analyzer is an object that is used to analyze text and break it down into
	 * tokens. It is used in this method to parse the synonym list and build the synonym map.
	 * @return The method is returning a {@code SynonymMap} object.
	 */
	public static SynonymMap loadSynonymList(final String synonymFile, Analyzer analyzer) {
		if (synonymFile == null)
			throw new NullPointerException("Synonym list file name cannot be null.");

		if (synonymFile.isEmpty())
			throw new IllegalArgumentException("Synonym list file name cannot be empty.");

		SolrSynonymParser synonymParser = new SolrSynonymParser(true, true, analyzer);
		SynonymMap synonymMap = null;
		try {
			Reader in = new BufferedReader(new InputStreamReader(CL.getResourceAsStream(synonymFile), "UTF-8"));
			synonymParser.parse(in);
			in.close();
			synonymMap = synonymParser.build();
		} catch (IOException | ParseException e) {
			throw new IllegalStateException(String.format(
					"Unable to load the synonym list %s: %s", synonymFile, e.getMessage()), e);
		}
		return synonymMap;
	}

	/**
	 * This function loads a plain text dictionary from a file and returns it.
	 * 
	 * @param dictionaryFile A {@code String} representing the name of the file containing the dictionary to be
	 * loaded.
	 * @return The method is returning a {@code PlainTextDictionary} object.
	 */
	public static PlainTextDictionary loadDictionary(final String dictionaryFile) {
		if (StringUtils.isBlank(dictionaryFile))
			throw new NullPointerException("Spellcheck dictionary file name is not valid.");

		PlainTextDictionary dict = null;
		try {
			Reader in = new BufferedReader(new InputStreamReader(CL.getResourceAsStream(dictionaryFile), "UTF-8"));
			dict = new PlainTextDictionary(in);
		} catch (IOException e) {
			throw new IllegalStateException(String.format(
					"Unable to load the spellcheck dict %s: %s", dictionaryFile, e.getMessage()), e);
		}
		return dict;
	}

	/**
	 * Loads the required stop list among those available in the {@code resources}
	 * folder.
	 *
	 * @param stopFile the name of the file containing the stop list.
	 *
	 * @return the stop list
	 *
	 * @throws IllegalStateException if there is any issue while loading the stop
	 *                               list.
	 */
	public static CharArraySet loadStopList(final String stopFile) {
		if (stopFile == null)
			throw new NullPointerException("Stop list file name cannot be null.");
		if (stopFile.isEmpty())
			throw new IllegalArgumentException("Stop list file name cannot be empty.");
		CharArraySet stopList = null;
		try {
			Reader in = new BufferedReader(new InputStreamReader(CL.getResourceAsStream(stopFile)));
			stopList = WordlistLoader.getWordSet(in);
			in.close();
		} catch (IOException e) {
			throw new IllegalStateException(
					String.format("Unable to load the stop list %s: %s", stopFile, e.getMessage()), e);
		}
		return stopList;
	}

	/**
	 * Loads the required Apache OpenNLP POS tagger model among those available in
	 * the {@code resources} folder.
	 *
	 * @param modelFile the name of the file containing the model.
	 *
	 * @return the required Apache OpenNLP model.
	 *
	 * @throws IllegalStateException if there is any issue while loading the model.
	 */
	static NLPPOSTaggerOp loadPosTaggerModel(final String modelFile) {

		if (modelFile == null) {
			throw new NullPointerException("Model file name cannot be null.");
		}

		if (modelFile.isEmpty()) {
			throw new IllegalArgumentException("Model file name cannot be empty.");
		}

		// the model
		NLPPOSTaggerOp model = null;

		try {

			// Get an input stream for the file containing the model
			InputStream in = new BufferedInputStream(CL.getResourceAsStream(modelFile));

			// Load the model
			model = new NLPPOSTaggerOp(new POSModel(in));

			// Close the file
			in.close();

		} catch (IOException e) {
			throw new IllegalStateException(String.format("Unable to load the model %s: %s", modelFile, e.getMessage()),
					e);
		}

		return model;
	}

	/**
	 * Loads the required Apache OpenNLP sentence detector model among those
	 * available in the {@code resources} folder.
	 *
	 * @param modelFile the name of the file containing the model.
	 *
	 * @return the required Apache OpenNLP model.
	 *
	 * @throws IllegalStateException if there is any issue while loading the model.
	 */
	static NLPSentenceDetectorOp loadSentenceDetectorModel(final String modelFile) {

		if (modelFile == null) {
			throw new NullPointerException("Model file name cannot be null.");
		}

		if (modelFile.isEmpty()) {
			throw new IllegalArgumentException("Model file name cannot be empty.");
		}

		// the model
		NLPSentenceDetectorOp model = null;

		try {

			// Get an input stream for the file containing the model
			InputStream in = new BufferedInputStream(CL.getResourceAsStream(modelFile));

			// Load the model
			model = new NLPSentenceDetectorOp(new SentenceModel(in));

			// Close the file
			in.close();

		} catch (IOException e) {
			throw new IllegalStateException(String.format("Unable to load the model %s: %s", modelFile, e.getMessage()),
					e);
		}

		return model;
	}

	/**
	 * Loads the required Apache OpenNLP tokenizer model among those available in
	 * the {@code resources} folder.
	 *
	 * @param modelFile the name of the file containing the model.
	 *
	 * @return the required Apache OpenNLP model.
	 *
	 * @throws IllegalStateException if there is any issue while loading the model.
	 */
	static NLPTokenizerOp loadTokenizerModel(final String modelFile) {

		if (modelFile == null) {
			throw new NullPointerException("Model file name cannot be null.");
		}

		if (modelFile.isEmpty()) {
			throw new IllegalArgumentException("Model file name cannot be empty.");
		}

		// the model
		NLPTokenizerOp model = null;

		try {

			// Get an input stream for the file containing the model
			InputStream in = new BufferedInputStream(CL.getResourceAsStream(modelFile));

			// Load the model
			model = new NLPTokenizerOp(new TokenizerModel(in));

			// Close the file
			in.close();

		} catch (IOException e) {
			throw new IllegalStateException(String.format("Unable to load the model %s: %s", modelFile, e.getMessage()),
					e);
		}

		return model;
	}

	/**
	 * Consumes a {@link TokenStream} for the given text by using the provided
	 * {@link Analyzer} and prints diagnostic
	 * information about all the generated tokens and their
	 * {@link org.apache.lucene.util.Attribute}s.
	 *
	 * @param a the analyzer to use.
	 * @param t the text to process.
	 *
	 * @throws IOException if something goes wrong while processing the text.
	 */
	public static void consumeTokenStream(final Analyzer a, final String t) throws IOException {

		// the start time of the processing
		final long start = System.currentTimeMillis();

		// Create a new TokenStream for a dummy field
		final TokenStream stream = a.tokenStream("field", new StringReader(t));

		// Lucene tokens are decorated with different attributes whose values contain
		// information about the token,
		// e.g. the term represented by the token, the offset of the token, etc.

		// The term represented by the token
		final CharTermAttribute tokenTerm = stream.addAttribute(CharTermAttribute.class);

		// The type the token
		final TypeAttribute tokenType = stream.addAttribute(TypeAttribute.class);

		// Whether the token is a keyword. Keyword-aware TokenStreams/-Filters skip
		// modification of tokens that are keywords
		final KeywordAttribute tokenKeyword = stream.addAttribute(KeywordAttribute.class);

		// The position of the token wrt the previous token
		final PositionIncrementAttribute tokenPositionIncrement = stream.addAttribute(PositionIncrementAttribute.class);

		// The number of positions occupied by a token
		final PositionLengthAttribute tokenPositionLength = stream.addAttribute(PositionLengthAttribute.class);

		// The start and end offset of a token in characters
		final OffsetAttribute tokenOffset = stream.addAttribute(OffsetAttribute.class);

		// Optional flags a token can have
		final SentenceAttribute sentenceAttribute = stream.addAttribute(SentenceAttribute.class);

		System.out.printf("####################################################################################%n");
		System.out.printf("Text to be processed%n");
		System.out.printf("+ %s%n%n", t);

		System.out.printf("Tokens%n");
		try {
			// Reset the stream before starting
			stream.reset();

			// Print all tokens until the stream is exhausted
			while (stream.incrementToken()) {
				System.out.printf("+ token: %s%n", tokenTerm.toString());
				System.out.printf("  - type: %s%n", tokenType.type());
				System.out.printf("  - keyword: %b%n", tokenKeyword.isKeyword());
				System.out.printf("  - position increment: %d%n", tokenPositionIncrement.getPositionIncrement());
				System.out.printf("  - position length: %d%n", tokenPositionLength.getPositionLength());
				System.out.printf("  - offset: [%d, %d]%n", tokenOffset.startOffset(), tokenOffset.endOffset());
				System.out.printf("  - sentence index: %d%n", sentenceAttribute.getSentenceIndex());
			}

			// Perform any end-of-stream operations
			stream.end();
		} finally {

			// Close the stream and release all the resources
			stream.close();
		}

		System.out.printf("%nElapsed time%n");
		System.out.printf("+ %d milliseconds%n", System.currentTimeMillis() - start);
		System.out.printf("####################################################################################%n");
	}
}
